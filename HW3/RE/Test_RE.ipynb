{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNrSKXmoJwlR",
        "outputId": "0c84300c-42f2-42e1-83ee-7aab1d425a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mendelai-brat-parser\n",
            "  Downloading mendelai_brat_parser-0.0.11.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mendelai-brat-parser\n",
            "  Building wheel for mendelai-brat-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mendelai-brat-parser: filename=mendelai_brat_parser-0.0.11-py3-none-any.whl size=4944 sha256=7dea413cf815d2dd623fd57594890a47e9be5009902426c880f744420cc341dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/a7/ff/138853d8196095fec56e0a97779a96d754b98f169c063beca3\n",
            "Successfully built mendelai-brat-parser\n",
            "Installing collected packages: mendelai-brat-parser\n",
            "Successfully installed mendelai-brat-parser-0.0.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.8/dist-packages (6.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mendelai-brat-parser in /usr/local/lib/python3.8/dist-packages (0.0.11)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install mendelai-brat-parser\n",
        "!pip install smart-open\n",
        "!pip install mendelai-brat-parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pyarrow.parquet as pq\n",
        "from brat_parser import get_entities_relations_attributes_groups\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torch_data\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "import pickle"
      ],
      "metadata": {
        "id": "qVNP1fnhJ0yX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.relation_classifier = nn.Sequential(\n",
        "            nn.Linear(3 * input_dim, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "        self.has_relation = nn.Sequential(\n",
        "            nn.Linear(3 * input_dim, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, cls_emb, subj_emb, obj_emb):\n",
        "        x = torch.cat([cls_emb, subj_emb, obj_emb], dim=1)\n",
        "        relation_logits = self.relation_classifier(x)\n",
        "        has_relation    = self.has_relation(x)\n",
        "        return relation_logits, has_relation"
      ],
      "metadata": {
        "id": "zivuUcZSLZUt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2label = {0: 'PPS',1: 'GOL',2: 'TSK',3: 'NPS',4: 'FNT',5: 'PNT',6: 'FNG',7: 'NNT',8: 'FPS',9: 'NNG',10: 'PNG',11: 'NO_RE'}\n",
        "label2idx = {v: k for k, v in idx2label.items()}"
      ],
      "metadata": {
        "id": "-eRXBOjNNaBE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ReEntity:\n",
        "    token: str\n",
        "    embeddings: list\n",
        "\n",
        "def infer_and_merge_embeddings(tokenizer, model, dataset, disable_tqdm=False):\n",
        "    model.eval()\n",
        "    def merge_tokens(tokens, embeddings=None):\n",
        "        assert embeddings is None or len(tokens.tokens()) == len(embeddings)\n",
        "        result = []\n",
        "        for i, token in enumerate(tokens.tokens()):\n",
        "            if token.startswith('##'):\n",
        "                result[-1].token += token.lstrip('##')\n",
        "                if embeddings is not None:\n",
        "                    result[-1].embeddings.append(embeddings[i].unsqueeze(0))\n",
        "            else:\n",
        "                result.append(ReEntity(token, []))\n",
        "                if embeddings is not None:\n",
        "                    result[-1].embeddings.append(embeddings[i].unsqueeze(0))\n",
        "        return tuple(zip(*list(map(lambda entity: (entity.token, torch.cat(entity.embeddings).mean(dim=0) if embeddings is not None else -1), result))))\n",
        "    \n",
        "    def get_sub_idx(find_in, to_find):\n",
        "        l1, l2 = len(find_in), len(to_find)\n",
        "        for i in range(l1):\n",
        "            if find_in[i:i+l2] == to_find:\n",
        "                return i\n",
        "        raise Exception(f\"Something went wrong. Cannot find {to_find} in {find_in}\")\n",
        "    result = []\n",
        "    bar = dataset if disable_tqdm else tqdm(dataset)\n",
        "    for text, subj, obj, is_subj_first, label in bar:\n",
        "        tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(**tokenized_text, output_hidden_states=True).hidden_states[-3][0]\n",
        "        tokenized_subj = tokenizer(subj)\n",
        "        tokenized_obj = tokenizer(obj)\n",
        "        \n",
        "        merged_tokens, merged_embeddings = merge_tokens(tokenized_text, embeddings)\n",
        "        merged_tokens, merged_embeddings = merged_tokens[:-1], merged_embeddings[:-1] # drop SEP\n",
        "        \n",
        "        merged_subj, _ = merge_tokens(tokenized_subj) \n",
        "        merged_obj, _ = merge_tokens(tokenized_obj)\n",
        "        \n",
        "        merged_subj = merged_subj[1:-1] # drop CLS and SEP\n",
        "        merged_obj = merged_obj[1:-1] # drop CLS and SEP\n",
        "        \n",
        "        cls_embed = merged_embeddings[0]\n",
        "        subj_start_idx = get_sub_idx(merged_tokens, merged_subj)\n",
        "        obj_start_idx = get_sub_idx(merged_tokens, merged_obj)\n",
        "        result.append((cls_embed, merged_subj, merged_embeddings[subj_start_idx:subj_start_idx + len(merged_subj)],\n",
        "                       merged_obj, merged_embeddings[obj_start_idx:obj_start_idx + len(merged_obj)], is_subj_first, label))\n",
        "    return result"
      ],
      "metadata": {
        "id": "XPirxmOdNlm9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def re_collate(batch):\n",
        "    (cls_embeds, subj, subj_embeds, obj, obj_embeds, is_subj_first, label) = zip(*batch)\n",
        "    clses = torch.stack(cls_embeds)\n",
        "    subjects_embs = torch.stack(list(map(lambda x: torch.stack(x).mean(dim=0), subj_embeds)))\n",
        "    objects_embs = torch.stack(list(map(lambda x: torch.stack(x).mean(dim=0), obj_embeds)))\n",
        "    labels = torch.tensor(list(map(lambda x: label2idx[x], label)))\n",
        "\n",
        "    return clses, subjects_embs, objects_embs, labels"
      ],
      "metadata": {
        "id": "hhQpXJzNNSgA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"surdan/LaBSE_ner_nerel\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"surdan/LaBSE_ner_nerel\")\n",
        "classifier = ReClassifier(768, 11)\n",
        "classifier.load_state_dict(torch.load(f'/content/drive/MyDrive/re_classifier_large_final_epochs=500_lr=1e-3_stepsize=250_gamma=0.1.model', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWHAizhgJ5s8",
        "outputId": "f131b996-7c84-42a4-e040-18b18236ce94"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_relations(classifier, bert, tokenizer, text, has_relation_treshold=0.5, visualizer=None):\n",
        "    def pack_dataset(text, subj, obj):\n",
        "        return [\n",
        "            (text, subj, obj, True, \"TSK\")\n",
        "        ]\n",
        "    ans = {}\n",
        "    classifier.eval()\n",
        "    bert.eval()\n",
        "    answer = []\n",
        "    tokenized = tokenizer(text)\n",
        "    tokens = tokenized.tokens()\n",
        "    seen = set()\n",
        "    for subj_idx, subj_token in enumerate(tokens):\n",
        "        if subj_idx == 0 or subj_idx == len(tokens):\n",
        "            continue\n",
        "        for obj_idx in range(subj_idx + 1, len(tokens) - 1):\n",
        "            subj_start, subj_end = tokenized.word_to_chars(tokenized.token_to_word(subj_idx))\n",
        "            obj_start, obj_end   = tokenized.word_to_chars(tokenized.token_to_word(obj_idx))\n",
        "            if subj_start == obj_start and subj_end == obj_end or ((subj_start, subj_end), (obj_start, obj_end)) in seen:\n",
        "                continue\n",
        "            seen.add(((subj_start, subj_end), (obj_start, obj_end)))\n",
        "            subj, obj = text[subj_start:subj_end], text[obj_start:obj_end]\n",
        "            packed = pack_dataset(text, subj, obj)\n",
        "            preprocess = infer_and_merge_embeddings(tokenizer, bert, packed, disable_tqdm=True)\n",
        "            clses, subjects_embs, objects_embs, labels = list(map(lambda x: x, re_collate(preprocess)))\n",
        "            with torch.no_grad():\n",
        "                relation_to, has_relation_to = classifier(clses, subjects_embs, objects_embs)\n",
        "                relation_from, has_relation_from = classifier(clses, objects_embs, subjects_embs)\n",
        "                has_relation_to_score = torch.sigmoid(has_relation_to).item()\n",
        "                has_relation_from_score = torch.sigmoid(has_relation_from).item()\n",
        "                if has_relation_to_score < has_relation_treshold and has_relation_from_score < has_relation_treshold:\n",
        "                    continue\n",
        "                is_to_relation = has_relation_to_score > has_relation_from_score\n",
        "                relation = relation_to if is_to_relation else relation_from\n",
        "                distribution = nn.functional.softmax(relation.squeeze(), dim=0)\n",
        "                label = distribution.argmax().item()\n",
        "                confidence = distribution[label].item()\n",
        "                if visualizer is not None:\n",
        "                    from_vert = f\"{subj}_{subj_idx}\" if is_to_relation else f\"{obj}_{obj_idx}\"\n",
        "                    to_vert = f\"{subj}_{subj_idx}\" if not is_to_relation else f\"{obj}_{obj_idx}\"\n",
        "                    visualizer.add_edge(from_vert, to_vert, idx2label[label])\n",
        "                # print(confidence, max(has_relation_to_score, has_relation_from_score), idx2label[label], subj, obj)\n",
        "                ans[subj + obj] = idx2label[label]\n",
        "    if visualizer is not None:\n",
        "        visualizer.visualize()\n",
        "    return ans"
      ],
      "metadata": {
        "id": "XXf6rmRWKbUP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_relations(classifier, model, tokenizer, \"Увеличить количество волонтеров анти  наркотической направленности\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FbxIfX-KcFB",
        "outputId": "480b54aa-a5af-46a4-c621-68827e55c470"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5827641487121582 1.0 FPS Увеличить количество\n",
            "0.9984435439109802 1.0 FPS Увеличить волонтеров\n",
            "0.9879553318023682 1.0 FPS Увеличить анти\n",
            "0.8079472780227661 0.9995242357254028 TSK Увеличить наркотической\n",
            "0.9861086010932922 1.0 NNT Увеличить направленности\n",
            "0.8610538840293884 1.0 FPS количество волонтеров\n",
            "0.9998779296875 0.9999986886978149 NPS количество анти\n",
            "0.9996383190155029 1.0 NPS количество направленности\n",
            "0.9999880790710449 1.0 NPS волонтеров анти\n",
            "1.0 0.9821747541427612 FPS волонтеров наркотической\n",
            "0.9177203178405762 1.0 NNG волонтеров направленности\n",
            "0.9653666019439697 0.9844523668289185 NNG наркотической направленности\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from brat_parser import get_entities_relations_attributes_groups\n",
        "\n",
        "entities, relations, attributes, groups = get_entities_relations_attributes_groups(\"/content/test.ann\")\n",
        "relations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eviyw7XPUCc",
        "outputId": "e757b525-e04b-426d-c481-20154b7570ea"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'R3': Relation(id='R3', type='NPS', subj='T6', obj='T5'),\n",
              " 'R4': Relation(id='R4', type='GOL', subj='T8', obj='T9'),\n",
              " 'R5': Relation(id='R5', type='GOL', subj='T11', obj='T12'),\n",
              " 'R6': Relation(id='R6', type='GOL', subj='T15', obj='T16'),\n",
              " 'R7': Relation(id='R7', type='GOL', subj='T19', obj='T20'),\n",
              " 'R9': Relation(id='R9', type='GOL', subj='T28', obj='T29'),\n",
              " 'R10': Relation(id='R10', type='GOL', subj='T30', obj='T31'),\n",
              " 'R11': Relation(id='R11', type='GOL', subj='T35', obj='T36'),\n",
              " 'R12': Relation(id='R12', type='GOL', subj='T38', obj='T39'),\n",
              " 'R13': Relation(id='R13', type='GOL', subj='T42', obj='T43'),\n",
              " 'R14': Relation(id='R14', type='GOL', subj='T45', obj='T46'),\n",
              " 'R15': Relation(id='R15', type='NNG', subj='T48', obj='T47'),\n",
              " 'R16': Relation(id='R16', type='NPS', subj='T53', obj='T52'),\n",
              " 'R17': Relation(id='R17', type='GOL', subj='T72', obj='T73'),\n",
              " 'R18': Relation(id='R18', type='GOL', subj='T75', obj='T76'),\n",
              " 'R19': Relation(id='R19', type='GOL', subj='T80', obj='T81'),\n",
              " 'R20': Relation(id='R20', type='NPS', subj='T87', obj='T86'),\n",
              " 'R21': Relation(id='R21', type='GOL', subj='T89', obj='T90'),\n",
              " 'R22': Relation(id='R22', type='GOL', subj='T93', obj='T94'),\n",
              " 'R23': Relation(id='R23', type='GOL', subj='T93', obj='T95'),\n",
              " 'R24': Relation(id='R24', type='GOL', subj='T96', obj='T97'),\n",
              " 'R25': Relation(id='R25', type='GOL', subj='T105', obj='T106'),\n",
              " 'R26': Relation(id='R26', type='GOL', subj='T114', obj='T115'),\n",
              " 'R27': Relation(id='R27', type='GOL', subj='T119', obj='T120'),\n",
              " 'R28': Relation(id='R28', type='GOL', subj='T122', obj='T123'),\n",
              " 'R29': Relation(id='R29', type='GOL', subj='T125', obj='T126'),\n",
              " 'R30': Relation(id='R30', type='TSK', subj='T139', obj='T140'),\n",
              " 'R31': Relation(id='R31', type='TSK', subj='T141', obj='T142'),\n",
              " 'R32': Relation(id='R32', type='GOL', subj='T146', obj='T147'),\n",
              " 'R33': Relation(id='R33', type='GOL', subj='T154', obj='T155'),\n",
              " 'R34': Relation(id='R34', type='GOL', subj='T163', obj='T159'),\n",
              " 'R36': Relation(id='R36', type='GOL', subj='T168', obj='T169'),\n",
              " 'R37': Relation(id='R37', type='TSK', subj='T173', obj='T172'),\n",
              " 'R1': Relation(id='R1', type='TSK', subj='T202', obj='T203'),\n",
              " 'R2': Relation(id='R2', type='GOL', subj='T251', obj='T252'),\n",
              " 'R8': Relation(id='R8', type='TSK', subj='T257', obj='T255'),\n",
              " 'R35': Relation(id='R35', type='TSK', subj='T257', obj='T256'),\n",
              " 'R38': Relation(id='R38', type='TSK', subj='T258', obj='T259'),\n",
              " 'R39': Relation(id='R39', type='TSK', subj='T263', obj='T261'),\n",
              " 'R40': Relation(id='R40', type='TSK', subj='T264', obj='T265'),\n",
              " 'R41': Relation(id='R41', type='TSK', subj='T274', obj='T275'),\n",
              " 'R42': Relation(id='R42', type='TSK', subj='T276', obj='T277'),\n",
              " 'R43': Relation(id='R43', type='TSK', subj='T278', obj='T279'),\n",
              " 'R44': Relation(id='R44', type='TSK', subj='T281', obj='T282'),\n",
              " 'R45': Relation(id='R45', type='TSK', subj='T283', obj='T285'),\n",
              " 'R46': Relation(id='R46', type='TSK', subj='T291', obj='T292'),\n",
              " 'R47': Relation(id='R47', type='TSK', subj='T293', obj='T294'),\n",
              " 'R48': Relation(id='R48', type='GOL', subj='T298', obj='T299'),\n",
              " 'R49': Relation(id='R49', type='TSK', subj='T307', obj='T306'),\n",
              " 'R50': Relation(id='R50', type='TSK', subj='T390', obj='T391'),\n",
              " 'R51': Relation(id='R51', type='TSK', subj='T429', obj='T431'),\n",
              " 'R52': Relation(id='R52', type='TSK', subj='T444', obj='T445'),\n",
              " 'R53': Relation(id='R53', type='TSK', subj='T488', obj='T489'),\n",
              " 'R54': Relation(id='R54', type='TSK', subj='T495', obj='T496'),\n",
              " 'R55': Relation(id='R55', type='TSK', subj='T514', obj='T515'),\n",
              " 'R56': Relation(id='R56', type='TSK', subj='T521', obj='T522'),\n",
              " 'R57': Relation(id='R57', type='GOL', subj='T570', obj='T571'),\n",
              " 'R58': Relation(id='R58', type='TSK', subj='T636', obj='T637'),\n",
              " 'R59': Relation(id='R59', type='TSK', subj='T632', obj='T633'),\n",
              " 'R60': Relation(id='R60', type='TSK', subj='T634', obj='T635'),\n",
              " 'R61': Relation(id='R61', type='GOL', subj='T645', obj='T646'),\n",
              " 'R62': Relation(id='R62', type='NPS', subj='T705', obj='T704'),\n",
              " 'R63': Relation(id='R63', type='NNT', subj='T707', obj='T706'),\n",
              " 'R64': Relation(id='R64', type='NNG', subj='T709', obj='T708'),\n",
              " 'R65': Relation(id='R65', type='NPS', subj='T713', obj='T712'),\n",
              " 'R66': Relation(id='R66', type='NNT', subj='T720', obj='T719'),\n",
              " 'R67': Relation(id='R67', type='NNG', subj='T728', obj='T2'),\n",
              " 'R68': Relation(id='R68', type='NNT', subj='T714', obj='T23'),\n",
              " 'R69': Relation(id='R69', type='NNT', subj='T721', obj='T4')}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import os\n",
        "\n",
        "def get_f1(filename):\n",
        "    seq_example = open(filename + \".txt\").read()\n",
        "    #calc preds\n",
        "    preds = find_relations(classifier, model, tokenizer, seq_example)\n",
        "    #read labels\n",
        "    entities, relations, attributes, groups = get_entities_relations_attributes_groups(filename + \".ann\")\n",
        "    labels = {}\n",
        "    for t in relations.values():\n",
        "        labels[entities[t.subj].text + entities[t.obj].text] = t.type\n",
        "    #calc f1\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    for j in preds.keys():\n",
        "        if labels.keys().__contains__(j) and labels[j] == preds[j] and labels[j] != \"NO_RE\":\n",
        "            tp += 1\n",
        "        if labels.keys().__contains__(j) and labels[j] == preds[j] and labels[j] == \"NO_RE\":\n",
        "            tn += 1\n",
        "        if labels.keys().__contains__(j) and  labels[j] != \"NO_RE\" and preds[j] == \"NO_RE\":\n",
        "            fn += 1\n",
        "        if labels.keys().__contains__(j) and  labels[j] == \"NO_RE\" and preds[j] != \"NO_RE\":\n",
        "            fp += 1\n",
        "        if not labels.keys().__contains__(j) and preds[j] == \"NO_RE\":\n",
        "            tn += 1\n",
        "        if not labels.keys().__contains__(j) and preds[j] != \"NO_RE\":\n",
        "            fp += 1\n",
        "    for j in labels.keys():\n",
        "        if preds.keys().__contains__(j):\n",
        "            continue\n",
        "        if labels[j] == \"NO_RE\":\n",
        "            tn += 1\n",
        "        if labels[j] != \"NO_RE\":\n",
        "            fp += 1\n",
        "    recall = tp / (tp + fn + 0.00001)\n",
        "    precision = tp / (tp + fp + 0.00001)\n",
        "    f1 = (2 * precision * recall) / (precision + recall + 0.00001)\n",
        "    return f1\n",
        "\n",
        "f1s = list()\n",
        "folder_name = \"test\"\n",
        "for path in os.listdir(folder_name):\n",
        "    if \".ann\" in path:\n",
        "        continue\n",
        "    f1s.append(get_f1(folder_name + \"/\" + path[:-4]))\n",
        "print(numpy.mean(f1s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqu-dzK6Uipj",
        "outputId": "9efa955f-55f2-4bbf-877d-8abe38d83e50"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.126836707\n"
          ]
        }
      ]
    }
  ]
}